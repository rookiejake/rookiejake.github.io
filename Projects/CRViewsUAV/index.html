<!DOCTYPE html>
<!-- created by MiyuKimura on May10, 2017 -->
<!-- Additions, formatting, and comments by Ben Clark, June 2018-->
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Cross Reality Collaboration Sandbox</title>
    <!-- Used to load in common html files used across the website. -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script>
        $(function () {
            $("#page-footer").load("../../modules/footer.html");
            $("#page-top").load("../../modules/page-top.html");
        });
    </script>
</head>

<body class="page-aboutpage-">
    <div id="page" class="hfeed site">
        <div id="page-top"></div>

        <div id="content" class="site-content">
            <div id="primary" class="content-area">
                <main id="main" class="site-main" role="main">
                    <h2 class="entry-title">Cross Reality Views Via an Unmanned Aerial Vehicle</h2>
                    <h5 style="text-align:right;">By Aaron Hitchcock</h5>
                    <h5 style="text-align:right;">April 2018</h5>

                    <h4>Abstract</h4>
                    <p>The fields of Augmented Reality (AR) and Virtual Reality (VR) have a consistent focus on
                        first person experiences. By utilizing wearable devices like the Microsoft HoloLens, Oculus
                        Rift
                        or Google Cardboard users can enter and interact with a virtual space. The Augmented Space
                        Library (ASL) [1], developed by Cross Reality Collaboration Sandbox (CRCS) Research group [2],
                        seeks to combine both physical and virtual spaces with virtual objects and allows multiple
                        users, both local and remote to interact. The capabilities of both these technologies can be
                        expanded by the use of a remote controlled camera allowing for the addition of third person or
                        remote first person viewing. This project creates a system and corresponding API allowing for
                        integration of these views into both AR and VR applications as well as the ASL system. The
                        system gives a user the ability to navigate and explore a remote physical space in real time or
                        see themselves and their surroundings in third person. From a functional perspective this
                        requires the remote control of a highly maneuverable camera. This was achieved through the
                        use of a Drone or UAV (Unmanned Aerial Vehicle). These new virtual views will allow for
                        AR/VR interaction in new ways as all prior physical points of view were bound to the user.
                    </p>

                    <iframe src="https://drive.google.com/file/d/1bzw0Un_wO8lxe1DXAp3pvrRXigqWmkYc/preview" width="1280"
                        height="720"></iframe>
                </main>
            </div>
        </div>
        <!-- Adds the page footer -->
        <div id="page-footer" class="footer-wrap clear "></div>
    </div>
</body>

</html>